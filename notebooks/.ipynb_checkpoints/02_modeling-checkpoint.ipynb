{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d16e4-f639-4049-bbe5-3d9204391156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc66e8-b85e-4eaf-b0eb-ea1180fbd3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = (Path().resolve() / \"..\" / \"data\" / \"train.csv\").resolve()\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "target_col = \"loss\"\n",
    "df[\"log_loss\"] = np.log1p(df[target_col])\n",
    "\n",
    "cat_cols = [c for c in df.columns if c.startswith(\"cat\")]\n",
    "cont_cols = [c for c in df.columns if c.startswith(\"cont\")]\n",
    "\n",
    "X = df[cat_cols + cont_cols]\n",
    "y = df[\"log_loss\"]\n",
    "\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51387b9-3b88-4971-8454-b27239fb5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(X_train.shape, X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fcc85-7ef3-43ba-975a-8d534b6cf9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"cont\", \"passthrough\", cont_cols),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee00615f-cdb7-4bd3-8774-300fb3c7debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", Ridge(alpha=10.0, random_state=42)),\n",
    "    ]\n",
    ")\n",
    "baseline_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd8858-6f39-4f87-8266-463153a66284",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.fit(X_train, y_train)\n",
    "pred_val_base = baseline_model.predict(X_val)\n",
    "\n",
    "mae_base = mean_absolute_error(y_val, pred_val_base)\n",
    "mse_base = mean_squared_error(y_val, pred_val_base)\n",
    "rmse_base = np.sqrt(mse_base)\n",
    "r2_base = r2_score(y_val, pred_val_base)\n",
    "\n",
    "print(\"Baseline (Ridge) on log_loss\")\n",
    "print(\"MAE:\", mae_base)\n",
    "print(\"RMSE:\", rmse_base)\n",
    "print(\"R^2:\", r2_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b23509-d9d5-419c-b068-47a061f8413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = y_val - pred_val_base\n",
    "plt.figure()\n",
    "plt.scatter(pred_val_base, resid, s=5)\n",
    "plt.axhline(0)\n",
    "plt.title(\"Baseline Residuals (log_loss)\")\n",
    "plt.xlabel(\"Predicted log_loss\")\n",
    "plt.ylabel(\"Residual (actual - pred)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8cd353-2c5f-47a3-bcd9-93d5f55b4b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 25000  # good starting point for 8GB\n",
    "rng = np.random.RandomState(42)\n",
    "idx = rng.choice(len(X_train), size=N, replace=False)\n",
    "\n",
    "X_train_s = X_train.iloc[idx]\n",
    "y_train_s = y_train.iloc[idx]\n",
    "\n",
    "print(\"Training sample:\", X_train_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ca5cf-5724-4097-8d0e-e4c7a8b4bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "gb_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", RandomForestRegressor(\n",
    "            n_estimators=80,        # keep small\n",
    "            max_depth=16,           # limits tree growth\n",
    "            max_features=0.3,       # reduces split cost\n",
    "            min_samples_leaf=20,    # smoother trees\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3c776-370c-457f-a4ac-692c1183363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model.fit(X_train_s, y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147b24e-5843-402c-84c7-0dc1300f22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_gb = gb_model.predict(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256a489-d7c2-4b8b-9c2a-44aead89c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "mae_gb = mean_absolute_error(y_val, pred_val_gb)\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_val, pred_val_gb))\n",
    "r2_gb = r2_score(y_val, pred_val_gb)\n",
    "\n",
    "print(\"RandomForest (sample-trained) on log_loss\")\n",
    "print(\"MAE:\", mae_gb)\n",
    "print(\"RMSE:\", rmse_gb)\n",
    "print(\"R^2:\", r2_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e9bd8-315c-4ab7-a464-4ed9699e54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Improvement vs baseline (MAE):\", (mae_base - mae_gb) / mae_base)\n",
    "print(\"Improvement vs baseline (RMSE):\", (rmse_base - rmse_gb) / rmse_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf89bcc-dba4-43ba-afcd-0588532e00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_val, pred_val_gb, s=5)\n",
    "plt.xlabel(\"Actual log_loss\")\n",
    "plt.ylabel(\"Predicted log_loss\")\n",
    "plt.title(\"Actual vs Predicted (log_loss) — Random Forest\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993af653-99f5-44f8-a0dc-42db4f9281c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decile_df = pd.DataFrame({\n",
    "    \"y_true_log\": y_val.values,\n",
    "    \"y_pred_log\": pred_val_gb\n",
    "})\n",
    "\n",
    "# Convert back to dollars\n",
    "decile_df[\"y_true_loss\"] = np.expm1(decile_df[\"y_true_log\"])\n",
    "decile_df[\"y_pred_loss\"] = np.expm1(decile_df[\"y_pred_log\"])\n",
    "\n",
    "# Create deciles based on predicted severity\n",
    "decile_df[\"decile\"] = pd.qcut(\n",
    "    decile_df[\"y_pred_loss\"], \n",
    "    10, \n",
    "    labels=False\n",
    ") + 1  # 1 = lowest risk, 10 = highest risk\n",
    "\n",
    "decile_summary = decile_df.groupby(\"decile\").agg(\n",
    "    count=(\"y_true_loss\", \"size\"),\n",
    "    avg_actual_loss=(\"y_true_loss\", \"mean\"),\n",
    "    avg_pred_loss=(\"y_pred_loss\", \"mean\"),\n",
    "    total_actual_loss=(\"y_true_loss\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "decile_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0e240-47b7-4700-a6eb-50ec173a2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(\n",
    "    decile_summary[\"decile\"],\n",
    "    decile_summary[\"avg_actual_loss\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Avg Actual Loss\"\n",
    ")\n",
    "plt.plot(\n",
    "    decile_summary[\"decile\"],\n",
    "    decile_summary[\"avg_pred_loss\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Avg Predicted Loss\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Predicted Severity Decile (1 = Lowest, 10 = Highest)\")\n",
    "plt.ylabel(\"Average Loss ($)\")\n",
    "plt.title(\"Decile Lift Chart — Random Forest Severity Ranking\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30f66f-f923-4e54-b6a1-bbeb88fcd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_loss = decile_summary[\"total_actual_loss\"].sum()\n",
    "\n",
    "top10_loss = decile_summary.loc[\n",
    "    decile_summary[\"decile\"] == 10, \"total_actual_loss\"\n",
    "].values[0]\n",
    "\n",
    "top20_loss = decile_summary.loc[\n",
    "    decile_summary[\"decile\"].isin([9,10]), \"total_actual_loss\"\n",
    "].sum()\n",
    "\n",
    "print(\"Share of total loss in top decile:\", top10_loss / total_loss)\n",
    "print(\"Share of total loss in top 2 deciles:\", top20_loss / total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5541df4-f580-4bb6-aa28-f5cf7a321b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract trained forest\n",
    "rf = gb_model.named_steps[\"model\"]\n",
    "\n",
    "# Get feature names from preprocessing\n",
    "ohe = gb_model.named_steps[\"preprocess\"].named_transformers_[\"cat\"]\n",
    "cat_feature_names = ohe.get_feature_names_out(cat_cols)\n",
    "feature_names = np.concatenate([cat_feature_names, cont_cols])\n",
    "\n",
    "importances = pd.Series(\n",
    "    rf.feature_importances_,\n",
    "    index=feature_names\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "importances.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3bd02-d5ab-44d8-a5cf-c08ed692a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "importances.head(15).iloc[::-1].plot(kind=\"barh\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Top Feature Drivers — Random Forest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf939d-2a1a-4558-8d48-b81a42cb83ca",
   "metadata": {},
   "source": [
    "Modeling Results & Interpretation\n",
    "Modeling Approach\n",
    "\n",
    "Severity was modeled using a log-transformed loss target to stabilize variance. Two models were evaluated:\n",
    "\n",
    "Ridge Regression as a linear baseline, chosen for stability and interpretability in high-dimensional categorical data\n",
    "\n",
    "Random Forest Regression, trained on a representative sample to explore non-linear effects while managing computational constraints\n",
    "\n",
    "Models were evaluated using both point-prediction metrics and decile-based loss concentration, aligning with real insurance decision-making workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca0097-e5d2-48a7-8539-d33e4d33bdd6",
   "metadata": {},
   "source": [
    "Performance Summary\n",
    "\n",
    "Baseline — Ridge Regression\n",
    "\n",
    "MAE: 0.440\n",
    "\n",
    "RMSE: 0.560\n",
    "\n",
    "R²: 0.519\n",
    "\n",
    "The baseline model performs strongly, capturing a substantial share of variance and providing a reliable reference point.\n",
    "\n",
    "Random Forest (Sample-Trained)\n",
    "\n",
    "MAE: 0.449\n",
    "\n",
    "RMSE: 0.574\n",
    "\n",
    "R²: 0.494\n",
    "\n",
    "The Random Forest does not outperform the linear baseline on global error metrics, suggesting that severity in this dataset is largely driven by additive effects rather than complex interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df31727-b1ec-4938-a6aa-faa4363bde7c",
   "metadata": {},
   "source": [
    "Ranking & Business Performance\n",
    "\n",
    "Despite weaker point accuracy, the Random Forest demonstrates strong risk ranking capability:\n",
    "\n",
    "Top decile captures ~25.7% of total loss\n",
    "\n",
    "Top two deciles capture ~41.5% of total loss\n",
    "\n",
    "This concentration supports operational use cases where prioritization and triage matter more than exact dollar prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa5bae-8b93-4c41-b763-4bc0065f8f9d",
   "metadata": {},
   "source": [
    "Modeling Takeaways\n",
    "\n",
    "Linear models provide stable severity estimates suitable for portfolio analysis\n",
    "\n",
    "Non-linear models add value through ranking and segmentation\n",
    "\n",
    "Insurance model evaluation should emphasize loss concentration, not just MAE/RMSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
